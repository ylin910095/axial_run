slurm_header:
    - "#SBATCH --account=axialgpu-20-21"
    - "#SBATCH --nodes=8"
    - "#SBATCH --ntasks-per-node=4"
    - "#SBATCH --time=6:00:00"
    - "#SBATCH --gres=gpu:4"
    - "#SBATCH --partition=long"
    - "#SBATCH --job-name=g5z"
    - "#SBATCH --mail-user=99liny@gmail.com"
    - "#SBATCH --mail-type=FAIL,TIME_LIMIT"
    - "#SBATCH --qos=normal"
    - "#SBATCH --output=/sdcc/u/ylin/lqcd/yin/data/a012_physical/slurmout/g5z_t0_%j.out"
module_command: 
    - module purge
    - module load anaconda2, gcc/8.2.0, openmpi/3.1.1-gnu, cuda/10.1
no_config: 1 # number of configurations running in parallel per job, usually just set it to 1
mpi_rank_per_node: 4 # make sure it matches slurm_header --nodes
openmp_thread_per_rank: 8 # make sure it matches slurm header --ntasks-per-node
ensemble: l4864f211b600m001907m05252m6382
QUDA_RESOURCE_PATH: /sdcc/u/ylin/axial/input_dir/a012_physical/tunefolder # tunning file folder for QUDA

projdir: /sdcc/u/ylin/axial/input_dir/axial_run # the directory of the scripts

# Data saving directory: 
# $datadir/corr for correlator, $datadir/prop for propagators, and so on
# if not directory not present, it will be created
datadir: /hpcgpfs01/work/lqcd/axial/yin/data/a012_physical # data saving directory
pythonprompt: pmt_2pt_strange.py # searched under projdir/pmt
rundb: rundb_a012_physical.sqlite # running database for all gauge configs, searched under projdir
milcbin: ks_spectrum_hisq_gb_baryon_blind_no_sink_links_newblind # searched under projdir

# Scratch directory for intermediate correlator files
# The script will create a unique scrach folder for each job within this folder
scratchdir: /hpcgpfs01/work/lqcd/axial/yin/temp/
